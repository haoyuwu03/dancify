{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Video Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Need opencv and cap_from_youtube installed\n",
        "# In anaconda env in terminal: \n",
        "#   pip install cap_from_youtube\n",
        "#   conda install -c conda-forge opencv\n",
        "# cap_from_youtube gets youtube video without downloading\n",
        "import cv2\n",
        "from cap_from_youtube import cap_from_youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=262hmHGMANE\n",
            "[youtube] 262hmHGMANE: Downloading webpage\n",
            "[youtube] 262hmHGMANE: Downloading android player API JSON\n"
          ]
        }
      ],
      "source": [
        "# User input:\n",
        "url = \"https://www.youtube.com/watch?v=262hmHGMANE\"\n",
        "#####################################################\n",
        "\n",
        "# Returns VideoCapture object\n",
        "capture = cap_from_youtube(url, '144p')\n",
        "\n",
        "# Get number of frames and frames per second\n",
        "num_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "frames = []\n",
        "\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = capture.read()\n",
        "    if ret:\n",
        "        #cv2.imshow('video', frame)\n",
        "        # waitKey miliseconds between frames defines how fast you can see video!!\n",
        "        #cv2.waitKey(1)\n",
        "        frames.append(frame)\n",
        "    if frame_count == num_frames:\n",
        "        break\n",
        "    frame_count += 1\n",
        "capture.release()\n",
        "#cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save frames if you don't want to re-load video\n",
        "# Large file can't be pushed to git though\n",
        "import pickle\n",
        "with open(\"frames\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(frames, fp)\n",
        "with open(\"frames\", \"rb\") as fp:   # Unpickling\n",
        "    frames_loaded = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frames was correctly saved and loaded\n"
          ]
        }
      ],
      "source": [
        "# Check frames was saved and loaded correctly\n",
        "import numpy as np\n",
        "frames = np.array(frames)\n",
        "frames_loaded = np.array(frames_loaded)\n",
        "if np.array_equal(frames,frames_loaded):\n",
        "    print('Frames was correctly saved and loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haoyuwu03/dancify/blob/main/Dancify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtXVKstlAKeX",
        "outputId": "698bc0e0-ac50-4bb4-b0ea-f58736b4d1cc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SFWeSatGARI"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from imutils.video import VideoStream\n",
        "import argparse\n",
        "import datetime\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "# construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-v\", \"--video\", help=\"drive/MyDrive/Videos/dance.MOV\")\n",
        "#ap.add_argument(\"-a\", \"--min-area\", type=int, default=500, help=\"minimum area size\")\n",
        "#args = vars(ap.parse_args())\n",
        "\n",
        "#reading from a video file\n",
        "vs = cv2.VideoCapture(\"drive/MyDrive/newDingga.mp4\")\n",
        "# initialize the first frame in the video stream\n",
        "firstFrame = vs.read()[1]\n",
        "frameWidth = firstFrame.shape[1]\n",
        "frameHeight = firstFrame.shape[0]\n",
        "# Change rate to slow down or speed up video (1=full speed)\n",
        "rate = 1\n",
        "fps = 30*rate # 30 fps is full speed\n",
        "vid_writer = cv2.VideoWriter('drive/MyDrive/dinggaDancify.mp4',cv2.VideoWriter_fourcc(*'MP4V'), fps, (frameWidth,frameHeight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hlfnfn7AsRg"
      },
      "outputs": [],
      "source": [
        "# Specify the paths for the 2 files\n",
        "protoFile = \"drive/MyDrive/Scale and Coin/models/pose/coco/pose_deploy_linevec.prototxt\"\n",
        "weightsFile = \"drive/MyDrive/Scale and Coin/models/pose/coco/pose_iter_440000.caffemodel\"\n",
        "# Read the network into Memory\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
        "# Using MPI model\n",
        "nPoints = 18\n",
        "# Specify the input image dimensions\n",
        "inWidth = 368\n",
        "inHeight = 368\n",
        "# Specify the threshold\n",
        "threshold = 0.01\n",
        "# Point pairs to draw line between for skeleton\n",
        "#Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4, Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, \n",
        "#Right Hip – 8, Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12, LAnkle – 13\n",
        "POSE_PAIRS = [ [1,0],[0,14],[0,15],[14,16],[15,17]]\n",
        "RIGHT_POSE_PAIRS = [[1,2],[2,3],[3,4],[1,8],[8,9],[9,10]]\n",
        "LEFT_POSE_PAIRS = [[1,5],[5,6],[6,7],[1,11],[11,12],[12,13]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2WpSfmKBH_s"
      },
      "outputs": [],
      "source": [
        "frames_before_update = 6 #number of frames before calculating new points\n",
        "frameCount = frames_before_update\n",
        "maxFrames = 5000\n",
        "count = 0\n",
        "last = -1\n",
        "# List to store detected keypoints\n",
        "points = []\n",
        "# loop over the frames of the video\n",
        "while True:\n",
        "    if count <= last:\n",
        "        continue\n",
        "    if count >= maxFrames:\n",
        "        break\n",
        "    count += 1\n",
        "    frameCount+=1\n",
        "    # grab the current frame\n",
        "    frame = vs.read()\n",
        "    frame = frame[1]\n",
        "    frameCopy = np.copy(frame)\n",
        "    # if the frame could not be grabbed, then we have reached the end\n",
        "\t# of the video\n",
        "    if frame is None:\n",
        "        break\n",
        "    if frameCount > frames_before_update:\n",
        "        frameCount = 0;\n",
        "        # Clear points to store new points\n",
        "        points = []\n",
        "        # Prepare the frame to be fed to the network\n",
        "        inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
        "                                    (0,0,0), swapRB=False, crop=False)\n",
        "        # Set the prepared object as the input blob of the network\n",
        "        net.setInput(inpBlob)\n",
        "\n",
        "        # Make predictions on where the body joints are\n",
        "        output = net.forward()\n",
        "\n",
        "        # Get height and width of prediction\n",
        "        H = output.shape[2]\n",
        "        W = output.shape[3]\n",
        "\n",
        "        for i in range(nPoints):\n",
        "            # confidence map of corresponding joint location\n",
        "            probMap = output[0, i, :, :]\n",
        "\n",
        "            # Find global maxima of the probMap\n",
        "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
        "\n",
        "            # Scale the point to fit on the original image\n",
        "            x = (frameWidth * point[0]) / W\n",
        "            y = (frameHeight * point[1]) / H\n",
        "\n",
        "            # If the probability is greater than the threshold, draw the point and add to list\n",
        "            if prob > threshold:\n",
        "                # CHANGE THESE LINES TO DESIGN POINTS\n",
        "                cv2.circle(frameCopy, (int(x),int(y)), 8, (0, 255, 255), thickness=1, \n",
        "                       lineType=cv2.FILLED)\n",
        "                cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                        1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
        "                points.append((int(x), int(y)))\n",
        "\n",
        "            else:\n",
        "                points.append(None)\n",
        "    # Draw skeleton between center points\n",
        "    \"\"\"for pair in POSE_PAIRS:\n",
        "        partA = pair[0]\n",
        "        partB = pair[1]\n",
        "\n",
        "        if points[partA] and points[partB]:\n",
        "            # CHANGE THESE LINES TO DESIGN SKELETON\n",
        "            cv2.line(frame, points[partA], points[partB], (224, 224, 224), 3, lineType=cv2.LINE_AA)\n",
        "            cv2.circle(frame, points[partA], 8, (178, 102, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "            cv2.circle(frame, points[partB], 8, (178, 102, 255), thickness=-1, lineType=cv2.FILLED)\"\"\"\n",
        "    # Draw skeleton between right points\n",
        "    for pair in RIGHT_POSE_PAIRS:\n",
        "        partA = pair[0]\n",
        "        partB = pair[1]\n",
        "\n",
        "        if points[partA] and points[partB]:\n",
        "            # CHANGE THESE LINES TO DESIGN SKELETON\n",
        "            cv2.line(frame, points[partA], points[partB], (255, 204, 153), 30, lineType=cv2.LINE_AA)\n",
        "            cv2.circle(frame, points[partA], 8, (255, 178, 102), thickness=-1, lineType=cv2.FILLED)\n",
        "            cv2.circle(frame, points[partB], 8, (255, 178, 102), thickness=-1, lineType=cv2.FILLED)\n",
        "    # Draw skeleton between left points\n",
        "    for pair in LEFT_POSE_PAIRS:\n",
        "        partA = pair[0]\n",
        "        partB = pair[1]\n",
        "\n",
        "        if points[partA] and points[partB]:\n",
        "            # CHANGE THESE LINES TO DESIGN SKELETON\n",
        "            cv2.line(frame, points[partA], points[partB], (255, 153, 204), 30, lineType=cv2.LINE_AA)\n",
        "            cv2.circle(frame, points[partA], 8, (255, 102, 178), thickness=-1, lineType=cv2.FILLED)\n",
        "            cv2.circle(frame, points[partB], 8, (255, 102, 178), thickness=-1, lineType=cv2.FILLED)\n",
        "    # Add text to frame\n",
        "    cv2.putText(frame, \"hey there sexy you're looking good ;)\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, .8, (255, 204, 153), 2, lineType=cv2.LINE_AA)\n",
        "\n",
        "    # Show frame\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Write frame to video\n",
        "    vid_writer.write(frame)\n",
        "\n",
        "vid_writer.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cm27vPHFuc"
      },
      "source": [
        "OLD MOTION DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSR1kSLSJEuh"
      },
      "outputs": [],
      "source": [
        "frameCount = 0\n",
        "firstFrame = None\n",
        "# loop over the frames of the video\n",
        "while True:\n",
        "    frameCount+=1\n",
        "    # grab the current frame\n",
        "    frame = vs.read()\n",
        "    frame = frame[1]\n",
        "    text = \"Unoccupied\"\n",
        "    # if the frame could not be grabbed, then we have reached the end\n",
        "\t# of the video\n",
        "    if frame is None:\n",
        "        break\n",
        "    # resize the frame, convert it to grayscale, and blur it\n",
        "    frame = imutils.resize(frame,width=500)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.GaussianBlur(gray,(21,21),0)\n",
        "    # if the first frame is None, initialize it\n",
        "    if firstFrame is None:\n",
        "        firstFrame = gray\n",
        "        continue\n",
        "    # compute the absolute difference between the current frame and\n",
        "\t# first frame\n",
        "    frameDelta = cv2.absdiff(firstFrame, gray)\n",
        "    thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n",
        "\t# dilate the thresholded image to fill in holes, then find contours\n",
        "\t# on thresholded image\n",
        "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
        "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = imutils.grab_contours(cnts)\n",
        "\t# loop over the contours\n",
        "    for c in cnts:\n",
        "\t\t# if the contour is too small, ignore it\n",
        "        if cv2.contourArea(c) < 500:\n",
        "            continue\n",
        "\t\t# compute the bounding box for the contour, draw it on the frame,\n",
        "\t\t# and update the text\n",
        "        (x, y, w, h) = cv2.boundingRect(c)\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        text = \"Dancer!\"\n",
        "    # draw the text and timestamp on the frame\n",
        "    #cv2.putText(frame, \"Room Status: {}\".format(text), (10, 20),\n",
        "\t#cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "    #cv2.putText(frame, datetime.datetime.now().strftime(\"%A %d %B %Y %I:%M:%S%p\"),\n",
        "\t#\t(10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
        "\t# show the frame and record if the user presses a key\n",
        "    #cv2_imshow(frame)\n",
        "    #cv2_imshow(thresh)\n",
        "    cv2_imshow(frameDelta)\n",
        "    #key = cv2.waitKey(1) & 0xFF\n",
        "    # if the `q` key is pressed, break from the lop\n",
        "    #if key == ord(\"q\"):\n",
        "    #    break\n",
        "# cleanup the camera and close any open windows\n",
        "#vs.release()\n",
        "#cv2.destroyAllWindows()\n",
        "print(frameCount)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Dancify.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "compsci527",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c3a6909629802760e1b80bc5544c2481223af25fceeb64eb2aabb12e10206c1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
